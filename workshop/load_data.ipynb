{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c10113d",
   "metadata": {},
   "source": [
    "### without dlt\n",
    "\n",
    "- Schema management is manual – If the schema changes, you need to update table structures manually \n",
    "- No automatic retries – If the network fails, data may be lost\n",
    "- No incremental loading – Every run reloads everything, making it slow and expensive\n",
    "- More code to maintain – A simple pipeline quickly becomes complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0452eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82545566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a connection to an in-memory DuckDB database\n",
    "conn = duckdb.connect(\"ny_taxi_manual.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d620d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Rides table\n",
    "conn.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS rides (\n",
    "    record_hash TEXT PRIMARY KEY,\n",
    "    vendor_name TEXT,\n",
    "    pickup_time TIMESTAMP,\n",
    "    dropoff_time TIMESTAMP,\n",
    "    start_lon DOUBLE,\n",
    "    start_lat DOUBLE,\n",
    "    end_lon DOUBLE,\n",
    "    end_lat DOUBLE\n",
    ");\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbc1275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert Data Manually\n",
    "# Since JSON data has nested fields, we need to extract and transform them before inserting them into DuckDB\n",
    "data = [\n",
    "    {\n",
    "        \"vendor_name\": \"VTS\",\n",
    "        \"record_hash\": \"b00361a396177a9cb410ff61f20015ad\",\n",
    "        \"time\": {\n",
    "            \"pickup\": \"2009-06-14 23:23:00\",\n",
    "            \"dropoff\": \"2009-06-14 23:48:00\"\n",
    "        },\n",
    "        \"coordinates\": {\n",
    "            \"start\": {\"lon\": -73.787442, \"lat\": 40.641525},\n",
    "            \"end\": {\"lon\": -73.980072, \"lat\": 40.742963}\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Prepare data for insertion\n",
    "flattened_data = [\n",
    "    (\n",
    "        ride[\"record_hash\"],\n",
    "        ride[\"vendor_name\"],\n",
    "        ride[\"time\"][\"pickup\"],\n",
    "        ride[\"time\"][\"dropoff\"],\n",
    "        ride[\"coordinates\"][\"start\"][\"lon\"],\n",
    "        ride[\"coordinates\"][\"start\"][\"lat\"],\n",
    "        ride[\"coordinates\"][\"end\"][\"lon\"],\n",
    "        ride[\"coordinates\"][\"end\"][\"lat\"]\n",
    "    )\n",
    "    for ride in data\n",
    "]\n",
    "\n",
    "# Insert into DuckDB\n",
    "conn.executemany(\"\"\"\n",
    "INSERT INTO rides (record_hash, vendor_name, pickup_time, dropoff_time, start_lon, start_lat, end_lon, end_lat)\n",
    "VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "\"\"\", flattened_data)\n",
    "\n",
    "print(\"Data successfully loaded into DuckDB!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e90432d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Data in DuckDB\n",
    "df = conn.execute(\"SELECT * FROM rides\").df()\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de64c1b",
   "metadata": {},
   "source": [
    "### with dlt \n",
    "\n",
    "- Supports multiple destinations (BigQuery, GCS, S3, Redshift, Snowflake, Postgres,...)\n",
    "- Optimized for performance – Uses batch loading, parallelism, and streaming for fast and scalable data transfer\n",
    "- Schema-aware – Ensures that column names, data types, and structures match the destination’s requirements\n",
    "- Incremental loading – Avoids unnecessary reloading by only inserting new or updated records\n",
    "- Resilience & retries – Automatically handles failures, ensuring data is loaded without missing records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1440bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "from dlt.sources.helpers.rest_client import RESTClient\n",
    "from dlt.sources.helpers.rest_client.paginators import PageNumberPaginator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9fc4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the API resource for NYC taxi data\n",
    "@dlt.resource(name=\"rides\") # will be used as the table name\n",
    "def ny_taxi():\n",
    "    client = RESTClient(\n",
    "        base_url=\"https://us-central1-dlthub-analytics.cloudfunctions.net\",\n",
    "        paginator=PageNumberPaginator(\n",
    "            base_page=1,\n",
    "            total_path=None\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for page in client.paginate(\"data_engineering_zoomcamp_api\"): # API endpoint for retrieving taxi ride data\n",
    "        yield page # yield data to manage memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7727ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define new dlt pipeline\n",
    "pipeline = dlt.pipeline(destination=\"duckdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7542eba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the pipeline with the new resource\n",
    "load_info = pipeline.run(ny_taxi, write_disposition=\"replace\")\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13ee81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore loaded data\n",
    "pipeline.dataset(dataset_type=\"default\").rides.df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90da52c9",
   "metadata": {},
   "source": [
    "### Incremental loading with dlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7804a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "from dlt.sources.helpers.rest_client import RESTClient\n",
    "from dlt.sources.helpers.rest_client.paginators import PageNumberPaginator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d1941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download only trips made after June 15, 2009, skipping the old ones (append)\n",
    "@dlt.resource(name=\"rides\", write_disposition=\"append\")\n",
    "def ny_taxi(\n",
    "    cursor_date=dlt.sources.incremental(\n",
    "        \"Trip_Dropoff_DateTime\",   # field to track, our timestamp\n",
    "        initial_value=\"2009-06-15\", # start date June 15, 2009\n",
    "        )\n",
    "    ):\n",
    "    client = RESTClient(\n",
    "        base_url=\"https://us-central1-dlthub-analytics.cloudfunctions.net\",\n",
    "        paginator=PageNumberPaginator(\n",
    "            base_page=1,\n",
    "            total_path=None\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for page in client.paginate(\"data_engineering_zoomcamp_api\"):\n",
    "        yield page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f1ecc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define new dlt pipeline\n",
    "# when run the second time, no new data will be loaded\n",
    "pipeline = dlt.pipeline(pipeline_name=\"ny_taxi\", destination=\"duckdb\", dataset_name=\"ny_taxi_data\")\n",
    "\n",
    "# run the pipeline with the new resource\n",
    "load_info = pipeline.run(ny_taxi)\n",
    "print(pipeline.last_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e478bd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query the earliest date in the loaded data\n",
    "with pipeline.sql_client() as client:\n",
    "    res = client.execute_sql(\n",
    "            \"\"\"\n",
    "            SELECT\n",
    "            MIN(trip_dropoff_date_time)\n",
    "            FROM rides;\n",
    "            \"\"\"\n",
    "        )\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e594bb7b",
   "metadata": {},
   "source": [
    "### Loading data into a Data Warehouse (BigQuery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32f6b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the dependencies\n",
    "!pip install dlt[bigquery]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d785924a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "from dlt.sources.helpers.rest_client import RESTClient\n",
    "from dlt.sources.helpers.rest_client.paginators import PageNumberPaginator\n",
    "import os\n",
    "import json\n",
    "from google.colab import userdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412ba59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign value for env var\n",
    "userdata = {}\n",
    "with open(\"credentials.json\", \"r\") as f:\n",
    "    userdata[\"BIGQUERY_CREDENTIALS\"] = json.dumps(json.load(f))\n",
    "\n",
    "os.environ[\"DESTINATION__BIGQUERY__CREDENTIALS\"] = userdata.get(\"BIGQUERY_CREDENTIALS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070f2889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload key to Google Colab (need to Create Service Account (assign BigQuery Admin & create key) first)\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562a7a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch data from API\n",
    "@dlt.resource(name=\"rides\", write_disposition=\"replace\")\n",
    "def ny_taxi():\n",
    "    client = RESTClient(\n",
    "        base_url=\"https://us-central1-dlthub-analytics.cloudfunctions.net\",\n",
    "        paginator=PageNumberPaginator(\n",
    "            base_page=1,\n",
    "            total_path=None\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for page in client.paginate(\"data_engineering_zoomcamp_api\"):\n",
    "        yield page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7e2bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipeline & run\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"taxi_data\",\n",
    "    destination=\"bigquery\",\n",
    "    dataset_name=\"taxi_rides\",\n",
    "    dev_mode=True,\n",
    ")\n",
    "\n",
    "info = pipeline.run(ny_taxi)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9362a476",
   "metadata": {},
   "source": [
    "### Loading data into a Data Lake (Parquet on Local FS or S3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1940cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dlt\n",
    "from dlt.sources.helpers.rest_client import RESTClient\n",
    "from dlt.sources.helpers.rest_client.paginators import PageNumberPaginator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59d861e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a local bucket or cloud directory for storing files\n",
    "os.environ[\"BUCKET_URL\"] = \"/content\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087fd9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch data from API\n",
    "@dlt.resource(name=\"rides\", write_disposition=\"replace\")\n",
    "def ny_taxi():\n",
    "    client = RESTClient(\n",
    "        base_url=\"https://us-central1-dlthub-analytics.cloudfunctions.net\",\n",
    "        paginator=PageNumberPaginator(\n",
    "            base_page=1,\n",
    "            total_path=None\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for page in client.paginate(\"data_engineering_zoomcamp_api\"):\n",
    "        yield page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8888ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipeline & run\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name='fs_pipeline',\n",
    "    destination='filesystem',\n",
    "    dataset_name='fs_data',\n",
    ")\n",
    "\n",
    "load_info = pipeline.run(ny_taxi, loader_file_format=\"parquet\") # choose a file format: parquet, csv or jsonl\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f60aabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore loaded data\n",
    "pipeline.dataset(dataset_type=\"default\").rides.df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c14de9",
   "metadata": {},
   "source": [
    "### Loading to Delta Lake or Iceberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1486cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"dlt[pyiceberg]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d16f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dlt\n",
    "from dlt.sources.helpers.rest_client import RESTClient\n",
    "from dlt.sources.helpers.rest_client.paginators import PageNumberPaginator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb749d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"BUCKET_URL\"] = \"/content\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0145fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch data from API\n",
    "@dlt.resource(name=\"rides\", write_disposition=\"replace\")\n",
    "def ny_taxi():\n",
    "    client = RESTClient(\n",
    "        base_url=\"https://us-central1-dlthub-analytics.cloudfunctions.net\",\n",
    "        paginator=PageNumberPaginator(\n",
    "            base_page=1,\n",
    "            total_path=None\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for page in client.paginate(\"data_engineering_zoomcamp_api\"):\n",
    "        yield page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edae2097",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name='fs_pipeline',\n",
    "    destination='filesystem',\n",
    "    dataset_name='fs_iceberg_data',\n",
    ")\n",
    "\n",
    "\n",
    "load_info = pipeline.run(\n",
    "    ny_taxi,\n",
    "    loader_file_format=\"parquet\",\n",
    "    table_format=\"iceberg\", \n",
    ")\n",
    "print(load_info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
